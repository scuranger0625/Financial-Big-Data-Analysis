{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AKJWr9GwIXM6YDyQRmd8bEZ6wdu5heOj",
      "authorship_tag": "ABX9TyPb9NODppepNVR/37IkOwHV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scuranger0625/Financial-Big-Data-Analysis/blob/main/%E5%A4%A7%E6%95%B8%E6%93%9A%E5%88%86%E6%9E%90%E4%BD%9C%E6%A5%AD2_Pyspark%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2：安裝跑PySpark的環境。"
      ],
      "metadata": {
        "id": "GRDGblRfPb8q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0fGFWt_FN4mQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e18b6f0-21a8-468d-99be-32b9ad65689f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libxtst6\n",
            "Suggested packages:\n",
            "  libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic fonts-ipafont-mincho\n",
            "  fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 30.8 MB of archives.\n",
            "After this operation, 104 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-8-jre-headless amd64 8u432-ga~us1-0ubuntu2~22.04 [30.8 MB]\n",
            "Fetched 30.8 MB in 1s (25.6 MB/s)\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 123632 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u432-ga~us1-0ubuntu2~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u432-ga~us1-0ubuntu2~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u432-ga~us1-0ubuntu2~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!apt-get -y install openjdk-8-jre-headless\n",
        "!pip install pyspark\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
        "sc = SparkContext.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3：使用Word Count 的範例程式，驗證PySpark環境是否可以"
      ],
      "metadata": {
        "id": "KLTGPJHEPfUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([\"Hello Spark\"])\n",
        "counts = rdd.flatMap(lambda line: line.split(\" \")) \\\n",
        "  .map(lambda word: (word, 1)) \\\n",
        "  .reduceByKey(lambda a, b: a + b) \\\n",
        "  .collect()\n",
        "print(counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PjWfPwGPTOx",
        "outputId": "fdfe0d87-c331-4f70-d68b-8b042c8f11c9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Hello', 1), ('Spark', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "安裝 findspark"
      ],
      "metadata": {
        "id": "QldvWduGQRLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9vPtbtqQC2B",
        "outputId": "0b60f2fe-db72-4347-ad55-5cea6168942d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import  findspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "findspark.init('/Users/pratikajitb/server/spark-2.4.0-bin-hadoop2.7')\n",
        "\n",
        "# creating the spark session\n",
        "spark = SparkSession.builder.appName('walmart').getOrCreate()\n",
        "\n",
        "df = spark.read.csv('walmart_stock.csv', inferSchema=True, header=True)\n",
        "\n",
        "df.columns\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUiyZBt9P5Af",
        "outputId": "cd839c25-6653-4674-a73b-2862c10ff439"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTrn46aARTHO",
        "outputId": "f8e9e195-b66d-406c-983c-e89ebccb9650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "印出前五個columns"
      ],
      "metadata": {
        "id": "AeDCbWkJRmT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for line in df.head(5):\n",
        "  print(line, '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgMxQdWbRrIK",
        "outputId": "4383179c-c5c7-4f50-d591-d9f6ccf5ae01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(Date=datetime.date(2012, 1, 3), Open=59.970001, High=61.060001, Low=59.869999, Close=60.330002, Volume=12668800, Adj Close=52.619234999999996) \n",
            "\n",
            "Row(Date=datetime.date(2012, 1, 4), Open=60.209998999999996, High=60.349998, Low=59.470001, Close=59.709998999999996, Volume=9593300, Adj Close=52.078475) \n",
            "\n",
            "Row(Date=datetime.date(2012, 1, 5), Open=59.349998, High=59.619999, Low=58.369999, Close=59.419998, Volume=12768200, Adj Close=51.825539) \n",
            "\n",
            "Row(Date=datetime.date(2012, 1, 6), Open=59.419998, High=59.450001, Low=58.869999, Close=59.0, Volume=8069400, Adj Close=51.45922) \n",
            "\n",
            "Row(Date=datetime.date(2012, 1, 9), Open=59.029999, High=59.549999, Low=58.919998, Close=59.18, Volume=6679300, Adj Close=51.616215000000004) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "利用describe()顯示DataFrame"
      ],
      "metadata": {
        "id": "ddTn_y4ESEyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIgWGn5MR8uU",
        "outputId": "75642a8e-ff51-4a1c-914d-21d1bfd47e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|summary|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|  count|              1258|             1258|             1258|             1258|             1258|             1258|\n",
            "|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
            "| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
            "|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
            "|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "描述性統計資料的資料框（describe()）中，平均值（mean）和標準差（stddev）的小數位數過多。請將這些數值格式化為僅顯示小數點後兩位。"
      ],
      "metadata": {
        "id": "XSeSAejKTL2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'''\n",
        " from pyspark.sql.types import (StructField, StringType, IntegerType, StructType)\n",
        " data_schema = [StructField('summary', StringType(), True), StructField('Open', StringType(), True),\n",
        " StructField('High', StringType(), True), StructField('Low', StringType(), True),\n",
        " StructField('Close', StringType(), True), StructField('Volume', StringType(), True),\n",
        " StructField('Adj Close', StringType(), True) ]\n",
        " final_struc = StructType(fields=data_schema)\n",
        "''' 舊版本程式碼 已淘汰?"
      ],
      "metadata": {
        "id": "tU2OmOpBTB3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = spark.read.csv('walmart_stock.csv', inferSchema=True, header=True)\n",
        "df.printSchema()\n",
        "#The schema given below is wrong, as it is mostly from an older version.\n",
        "#Spark is able to predict the schema correctly now"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQtz8nqgSQEa",
        "outputId": "c10edc7e-7194-4d09-baf4-431d13c555e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import format_number\n",
        "\n",
        "# 顯示其DataFrame\n",
        "summary = df.describe()\n",
        "summary.select(summary['summary'],\n",
        "              format_number(summary['Open'].cast('float'), 2).alias('Open'),\n",
        "              format_number(summary['High'].cast('float'), 2).alias('High'),\n",
        "              format_number(summary['Low'].cast('float'), 2).alias('Low'),\n",
        "              format_number(summary['Close'].cast('float'), 2).alias('Close'),\n",
        "              format_number(summary['Volume'].cast('int'),0).alias('Volume')\n",
        "              ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKV6aviMTX82",
        "outputId": "1a689e95-441c-43cb-854d-1ee51c758af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+--------+--------+--------+----------+\n",
            "|summary|    Open|    High|     Low|   Close|    Volume|\n",
            "+-------+--------+--------+--------+--------+----------+\n",
            "|  count|1,258.00|1,258.00|1,258.00|1,258.00|     1,258|\n",
            "|   mean|   72.36|   72.84|   71.92|   72.39| 8,222,093|\n",
            "| stddev|    6.77|    6.77|    6.74|    6.76| 4,519,780|\n",
            "|    min|   56.39|   57.06|   56.30|   56.42| 2,094,900|\n",
            "|    max|   90.80|   90.97|   89.25|   90.47|80,898,100|\n",
            "+-------+--------+--------+--------+--------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下開始是作業2的內容"
      ],
      "metadata": {
        "id": "QLhGhutrUCoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new dataframe with a column called HV Ratio that is the ratio of the High Price versus volume of\n",
        "stock traded for a day."
      ],
      "metadata": {
        "id": "RDx05THWUZIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_hv = df.withColumn(\"HV Ratio\", df['High']/df['Volume'])\n",
        "df_hv.select('HV Ratio').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "td0zpVqLUG-V",
        "outputId": "152bc4d2-007e-48d0-ce2f-1689ae6c946b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|            HV Ratio|\n",
            "+--------------------+\n",
            "|4.819714653321546E-6|\n",
            "|6.290848613094555E-6|\n",
            "|4.669412994783916E-6|\n",
            "|7.367338463826307E-6|\n",
            "|8.915604778943901E-6|\n",
            "|8.644477436914568E-6|\n",
            "|9.351828421515645E-6|\n",
            "| 8.29141562102703E-6|\n",
            "|7.712212102001476E-6|\n",
            "|7.071764823529412E-6|\n",
            "|1.015495466386981E-5|\n",
            "|6.576354146362592...|\n",
            "| 5.90145296180676E-6|\n",
            "|8.547679455011844E-6|\n",
            "|8.420709512685392E-6|\n",
            "|1.041448341728929...|\n",
            "|8.316075414862431E-6|\n",
            "|9.721183814992126E-6|\n",
            "|8.029436027707578E-6|\n",
            "|6.307432259386365E-6|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What day had the Peak High in Price?"
      ],
      "metadata": {
        "id": "6kPmo-34Uaxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U8a-KfieURQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.orderBy(df['High'].desc()).select(['Date']).head(1)[0]['Date']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Msmykgv7Udvs",
        "outputId": "aeaa1128-e490-405a-fa55-e00e5c093083"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.date(2015, 1, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "How many days was the Close lower than 60 dollars?\n",
        "\n",
        "\n",
        "有多少天的「Close」低於 60 美元？"
      ],
      "metadata": {
        "id": "C7-4LLWsVHdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用 PySpark 的 DataFrame API 篩選並計算資料筆數\n",
        "\n",
        "# 1. df['Close'] < 60\n",
        "#    - 篩選條件：選出 'Close' 欄位中小於 60 的所有資料。\n",
        "#    - 這是一個條件過濾操作，返回符合條件的資料子集。\n",
        "\n",
        "# 2. df.filter(df['Close'] < 60)\n",
        "#    - 應用篩選條件，生成新的 DataFrame，僅包含 'Close' 值小於 60 的行。\n",
        "\n",
        "# 3. .count()\n",
        "#    - 計算篩選後的 DataFrame 中的總行數，即滿足條件的資料筆數。\n",
        "\n",
        "# 結果：\n",
        "# 該程式碼返回一個整數，表示 'Close' 欄位值小於 60 的資料筆數。\n",
        "\n",
        "df.filter(df['Close']<60).count()"
      ],
      "metadata": {
        "id": "WcpcWpTlVMiU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3960f48-0172-4908-e8c8-a4996e69abf3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "「Close」欄位的平均值是多少？\n",
        "What is the mean of the Close column?"
      ],
      "metadata": {
        "id": "KuUltGUrzmdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean\n",
        "\n",
        "df.select(mean('Close')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZiJYMmoy6da",
        "outputId": "f15184f4-aae8-4545-edc5-358714d71c54"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|       avg(Close)|\n",
            "+-----------------+\n",
            "|72.38844998012726|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the max and min of the Volume column?"
      ],
      "metadata": {
        "id": "iOxrIgrRzhnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import min, max\n",
        "df.select(max('Volume'),min('Volume')).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nSIgM84zdTP",
        "outputId": "8b788f9a-0db0-4957-f634-f50c37a88c70"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "|max(Volume)|min(Volume)|\n",
            "+-----------+-----------+\n",
            "|   80898100|    2094900|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What percentage of the time was the High greater than 80 dollars ?\n",
        " In other words, (Number of Days High>80)/(Total Days in the dataset)\n",
        " 高於80美元的天數佔總天數的百分比是多少？ 換句話說，（高於80美元的天數）/（數據集中的總天數）\n",
        ""
      ],
      "metadata": {
        "id": "GxzmuUcLzypx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter('High > 80').count() * 100/df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4-Gy-EZz7uL",
        "outputId": "0193359d-5870-440a-c8b1-a7714768bb7e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.141494435612083"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the Pearson correlation between High and Volume?\n",
        "高價和成交量之間的皮爾遜相關係數是多少？"
      ],
      "metadata": {
        "id": "qxPVc2gC0ZqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import corr\n",
        "df.select(corr(df['High'], df['Volume'])).show()\n",
        "\n",
        "df.corr('High', 'Volume')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUzT7Fhw0b4w",
        "outputId": "93e7d06f-108d-4050-d670-69a59da820fc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "| corr(High, Volume)|\n",
            "+-------------------+\n",
            "|-0.3384326061737161|\n",
            "+-------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.3384326061737161"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the max High per year?\n",
        "每年的最高價是多少？"
      ],
      "metadata": {
        "id": "L7TuegyC0rZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import (dayofmonth, hour,\n",
        " dayofyear, month,\n",
        " year, weekofyear,\n",
        " format_number, date_format)\n",
        "year_df = df.withColumn('Year', year(df['Date']))\n",
        "year_df.groupBy('Year').max()['Year', 'max(High)'].show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX2YDxGG0tfT",
        "outputId": "7db319c9-6801-4173-fb14-7604434598e5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|Year|max(High)|\n",
            "+----+---------+\n",
            "|2015|90.970001|\n",
            "|2013|81.370003|\n",
            "|2014|88.089996|\n",
            "|2012|77.599998|\n",
            "|2016|75.190002|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " What is the average Close for each Calendar Month?\n",
        " 每個日曆月份的平均收盤價是多少？"
      ],
      "metadata": {
        "id": "OCBHOElW04Es"
      }
    },
    {
      "source": [
        "from pyspark.sql.functions import (dayofmonth, hour,\n",
        " dayofyear, month,\n",
        " year, weekofyear,\n",
        " format_number, date_format)\n",
        "\n",
        "#Create a new column Month from existing Date column\n",
        "month_df = df.withColumn('Month', month(df['Date']))\n",
        "\n",
        "#Group by month and take average of all other columns\n",
        "month_df = month_df.groupBy('Month').mean()\n",
        "\n",
        "#Sort by month\n",
        "month_df = month_df.orderBy('Month')\n",
        "\n",
        "#Display only month and avg(Close), the desired columns\n",
        "month_df['Month', 'avg(Close)'].show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKvA2UPJ3f0V",
        "outputId": "fe16aa77-6a09-46ea-e0c7-02431b01f1a3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+\n",
            "|Month|       avg(Close)|\n",
            "+-----+-----------------+\n",
            "|    1|71.44801958415842|\n",
            "|    2|  71.306804443299|\n",
            "|    3|71.77794377570092|\n",
            "|    4|72.97361900952382|\n",
            "|    5|72.30971688679247|\n",
            "|    6| 72.4953774245283|\n",
            "|    7|74.43971943925233|\n",
            "|    8|73.02981855454546|\n",
            "|    9|72.18411785294116|\n",
            "|   10|71.57854545454543|\n",
            "|   11| 72.1110893069307|\n",
            "|   12|72.84792478301885|\n",
            "+-----+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import year, max, min, col, corr\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Walmart Stock Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'walmart_stock.csv'  # Replace with the correct file path\n",
        "df = spark.read.csv(file_path, inferSchema=True, header=True)\n",
        "\n",
        "# Calculate max price spread (max High - min Low) per year\n",
        "df_with_year = df.withColumn(\"Year\", year(col(\"Date\")))\n",
        "max_spread = df_with_year.groupBy(\"Year\").agg(\n",
        "    (max(\"High\") - min(\"Low\")).alias(\"Max_Price_Spread\")\n",
        ")\n",
        "\n",
        "# Calculate Pearson correlation between Volume and HV Ratio\n",
        "df_with_hv_ratio = df.withColumn(\"HV Ratio\", col(\"High\") / col(\"Volume\"))\n",
        "pearson_corr = df_with_hv_ratio.select(\n",
        "    corr(\"Volume\", \"HV Ratio\").alias(\"Pearson_Correlation\")\n",
        ")\n",
        "\n",
        "# Display results\n",
        "max_spread.show()\n",
        "pearson_corr.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFVNl6u-1kzB",
        "outputId": "ea0b18c9-7975-46a1-ff96-8ab21e1db029"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------------+\n",
            "|Year|  Max_Price_Spread|\n",
            "+----+------------------+\n",
            "|2015|         34.670002|\n",
            "|2013|         13.650002|\n",
            "|2014|15.819998999999996|\n",
            "|2012|         20.419998|\n",
            "|2016|14.990001000000007|\n",
            "+----+------------------+\n",
            "\n",
            "+-------------------+\n",
            "|Pearson_Correlation|\n",
            "+-------------------+\n",
            "|-0.7206777102718382|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}